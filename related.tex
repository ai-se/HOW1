

\section{Related Work}

 

HOW is more a data mining method than a model-based optimizer. It represents our next attempt
to develop model-less instance-based optimizers.  Our previous attempts in this area
were the W2, WHERE, and IDEA systems~~\cite{menzies13:brady,Menzies2013:local,me12c}.
W2 was developed with Brady et al.~\cite{menzies13:brady}. 
W2 reasoned over deltas in the the raw
attributes values (and not the synthesized attributes used by HOW). 
The results of  adjusting values via those deltas was assessed via an SQL-like
query that selected all examples in the database that satisfied the newly adjusted
values. W2 suffers from two problems that are resolved
in this paper: {\em optimization failure} and {\em instance exhastion}.
\bi
\item 
As reported in that paper, W2
had a large number of {\em optimization failures}; i.e. after applying its recommendations,
the objective scores actually got worse. We recommend HOW's approach (of reasoning
over the synthesized attributes) to W2 since {\em in none of the above
results} do we observe that {\em after} we treat the data, that the objective scores are worse.
\item
As to {\em instance exhaustion}, W2 was a simple case-based reasoning system that never generalized
over its instances. Hence, the SQl-like query used to asses W2's plans often returned
a vanishingly small number of instances. To avoid this problem, we strongly recommend
combining a planning system with a prediction system (as done in this paper) such that we can
generalize to adjusted examples not seen on the original sample.
\ei
The WHERE tool~\cite{Menzies2013:local} developed the recursive clustering method over synthesized attributes
  used in this paper. WHERE was   a prediction
system since reported current patterns in the data, rather than proposing plans
on how to improve those patterns. Nor did that tool offer guidance on how to assess
the results of stepping along those patterns to guide changes to the system.

The problems of W2 and the success of WHERE suggested in might be useful to combine
WHERE's synthesized attributes with W2. The result was the IDEA tool~\cite{me12c}. 
While the initial prototype seemed promising~\cite{me12c}, we made the mistake of using
the tree structure of the recursive clusters to generate plans (in IDEA, the plan to
move an instance from one poorly performing cluster $C_1$ to another $C_2$ was computed
from the different in the  median
values of the instances in the two  branches that lead to $C_1$ and $C_2$).  When
that approach was applied to the data used in this paper, the results were quite weak
(very little net improvement). However, we abandoned the tree-query approach
and went for the pure instance-based approach discussed at the start of this paper.


Moving on to other releated work, depending on the audience  we would
 call HOW a {\em spectral learner}, 
a {\em response surface method}, or  a {\em local search operator}.
According to
Kamvar et al~\cite{kamvar03}
{\em spectral learners}reason across $m$ underlying dimensions synthesized
using (say) PCA~\cite{pearson1901}. For example,
algorithms like PDDP~\cite{boley98} use PCA to
recursively divide data into smaller regions where each level of recursion needs an   $O(N^2)$ analysis
to infer the principle components. HOW performs an analogous procedure using a   $O(2N)$ analysis
based on a heuristic proposed by
Faloutsos and Lin~\cite{Faloutsos1995} (see above, the ``FASTMAP'' algorithm).

{\em Response surface methods}  build a fast approximation to the function being optimized,
then uses that  approximation to take intelligent guesses about useful mutations. HOW
is a non-parametric response surface method that finds its response surface via recursive
clustering. For an example of a parametric response surface method, that uses Gaussian process
models, see Zuluaga et al.~\cite{zuluaga13}.

A {\em local search operator} that a some probability $p$, nudges  instance variables
along a dimension that improves its objective scores. HOW's recursive clustering methods
combine $n$ dimensions into $m \ll n$ so its ``nudges'' are simultaneous
adjustments along multiple dimensions. For an example of other kinds of local search algorithms, that only adjust
one variable at a time, see the literature on WALKSAT~\cite{Selman1992} and MAXWALKSAT~\cite{kautz97}.
See also the literature on local search methods in multi-objective optimization.
For example,
Peng et al.~\cite{peng09:ls} have augmented MOEAs with
local search  (i.e. applying a problem-specific repair/improvement
heuristic on some current solution).
Also, Igel et al.'s~\cite{igel07} multi-objective
covariance matrix adaptation evolution strategy
can run the mutations along  ``ridges'' in the search space.

Note that the above response surface methods and local search operators cannot be compared to HOW
since, unlike HOW, they assume the presence of some model that can evaluate generations of newly mutated
instances. Recall from the above that the goal of HOW is to offer an optimization method without
using the evolutionary methods that make extensive demands on an underlying domain model.



 
 \subsubsection{Aside: Can We Call These Plans ``Causal''?}
With one additional stipulation,   this framework can be used to check if the learned
plans can be said to {\em cause} a change in the domain being explored.

In the literature, there are two main definitions of causality: Hill's H-causality~\cite{Hill1965}
and Granger's G-casualty~\cite{granger80}. 
An effect
is H-causal if it satisfies nine criteria\footnote{Hill's criteria for
demonstrating causality:
{\em  plausibility};
{\em strength} of the effect;
{\em 
specificity}, i.e. locality of the effect
to specialized groups;
{\em
support} by experimental evidence;
{\em 
coherence} between cause-effects noted
in the field and in laboratory samples;
{\em
temporally} i.e. the effect has to follow the cause;
{\em 
analogy}, i.e.  similar causes have similar effects;
{\em
biological gradient}, i.e.  more exposure to the cause leads to more of an
effect; and
{\em
consistency}, i.e.  the same cause-effect pairings have been seen by different persons
in different places.}
It is hard to demonstrate H-causality in software engineering.
For example, Hill's criteria emphasises repeatability and laboratory studies-- both of which are
hard to perform in software engineering (e.g. see the continued debate about
the value of using undergraduate students to surrogates for industrial
practitioners in software engineering research~\cite{Carver2003}). Also, Hill's requirement
for consistent results is far easier to achieve in studies of small closed systems (e.g. bacteria in a petri dish)
than in large socio-technical systems (e.g. open source projects). In the latter,
effects are more {\em probabilistic} than {\em categorical}-- by which we mean that 
some cause tends to cause some effect at some probability $p$ less that one.
This in turn means that at some probability $q$ (where $0 \le q < p \le 1$)
that we will find examples of {\em inconsistency} where some cause does {\em not} lead
to some effect (thus violating one of Hill's criteria for demonstrating causality).

The  difficulties associated with Hill's criteria lead Granger~\cite{granger80}
to propose another more practical criteria\footnote{Granger  himself comments that
his proposal was based on earlier work by Norbert Weiner~\cite{Seth2007}.}.
\mbox{G-causality} is based on building predictions based on {\em past} data,
then showing that this works for {\em future} data~\cite{granger80}. 
 In  \fig{work}, we demand that the {\em test}
data is not necessarily collected before the {\em train} data:
\bi
\item[\#1:] For data collected from software released in multiple versions, we should hence use  versions  $i,j$ to build models that are applied to version  $k$ where $i<j<k$.
\item[\#2:] For data collected from Monte Carlo simulations (where inputs are selected at random)
then as long as simulation $i$ is not used to adjust simulation $j> i$, then any item in that
data could be generated before any other. For this kind of data, we  can use the first $X\%$
of the simulation for training, and the remaining data for testing.
\ei
In the two case studies of this paper, the Jureczko object-oriented defect data comes from software
released in multiple versions while the Seigmund configuration data comes from Monte Carlo simulations.
Hence, we use procedure \#1 for the defect data and procedure \#2 for the configuration data.

With these changes,
we can reasonably claim that the plan's generating by HOW are G-causal.