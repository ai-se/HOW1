\documentclass[conference]{IEEEtran}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{tabulary}
\usepackage{bigstrut}
\setlength\fboxsep{1pt}
\setlength\fboxrule{1pt}
\usepackage{multicol}
\bstctlcite{IEEEexample:BSTcontrol}
\usepackage[table]{xcolor}
\usepackage{picture}
\newcommand{\keyword}[1]{\textit{#1}}
\newcommand{\quart}[4]{\begin{picture}(100,3)
  {\color{black}\put(#3,3){\circle*{4}}\put(#1,3){\line(1,0){#2}}}\end{picture}}
\usepackage{amsmath}
\usepackage{balance}
\usepackage{flushend}
\usepackage[english]{babel}
\usepackage{blindtext}
\usepackage{times}
\usepackage{cite}
\usepackage{hyperref}
\hypersetup{
  colorlinks = false,
  hidelinks = true
}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\begin{document}
  \title{An Instance Based Approach for Empirical Transfer Learning}
  
  \author{\IEEEauthorblockN{Rahul Krishna}
    \IEEEauthorblockA{
      North Carolina State University, USA\\
      rkrish11@ncsu.edu}
    \and
    \IEEEauthorblockN{Tim Menzies}
    \IEEEauthorblockA{North Carolina State University, USA\\
      tim.menzies@gmail.com}}
  
  % make the title area
  \maketitle
  
  
  \begin{abstract}
 
  \end{abstract}
  \begin{IEEEkeywords}
    defect prediction, CART
  \end{IEEEkeywords}
  
\section{Introduction}
\section{Motivating Example}
\section{Background}
\section{Instance Based Learning}
Assessing the performance of certain techniques can be extremely hard. In several applications we have models that can emulate the performance of systems. Using these models it is possible to examine several scenarios in a reproducible manner. However, models aren't always the solution, as we shall see. 

There exists several problems where models are hard to obtain, or the input and output are related by complex connections that simply cannot be modeled in a reliable manner, or generation of reliable models take prohibitively long. Software defect prediction is an excellent example of such a case. Models that incorporate all the intricate issues that may lead defects in a product is extremely hard to come by. In this paper we propose the use of an instance based approach in place of the conventional model based approach.

\subsection{Spectral Learning using WHERE}
The algorithm uses WHERE to recursively cluster the input data to identify subsets in the training data that a test instance can learn from. WHERE is a spectral learner which uses the FastMap heuristic to estimate the first principal component. It then recursively partitions the data into two halves along the median point of the projection on the first principal component, terminating when a half has less than $\sqrt{N}$ items.   

\subsection{The Planning Algorithm}
The planning scheme makes use of the clusters generated using the WHERE algorithm discussed above. Following this, a nearest neighbour scheme is applied to identify pairs of nearby clusters. A projective plane is constructed with these clusters at the vertices to characterize the cluster pairs.

The mutation policy works by projecting the test instance onto the projective planes and identifying the the plane on which the test instance has the largest scalar projection. The planner then reflects over the vertices of the chosen hyperplane, identifying the \textit{better} vertex among the two. Now, a new instance is generated by mutating the attributes of the test instance towards the better vertex and away from the worse vertex. This process is repeated for all the test instances that are considered defective.
\section{Experimental Design}
The following section describes the experiments used to measure the performance of WHAT on 10 defect data sets and 6 performance prediction data sets.
\subsection{Data sets}
The data was obtained from the PROMISE repository. For the defect data we investigated 32 releases from 11 open source Java projects defined by the metrics highlighted in \ref{}: \keyword{Apache Ant} (1.5 -- 1.7), \keyword{Apache Camel} (1.2 -- 1.6), \keyword{Apache Ivy} (1.1 -- 2.0), \keyword{JEdit} (4.1 -- 4.3), \keyword{Apache Log4j} (1.0 -- 1.2), \keyword{Apache Lucene} (2.0 -- 2.2), \keyword{PBeans} (1.0 and 2.0), \keyword{Apache POI} (2.0 -- 3.0), \keyword{Apache Synapse} (1.0 -- 1.2), \keyword{Apache Velocity} (1.4 -- 1.6), and \keyword{Apache Xalan-Java} (2.5 -- 2.7). Given the empirical nature of the data, it is important to design an experiment such that the planning phase uses only the \keyword{past} data to learn trends which can then be applied to the \keyword{future} data. Thus for our experiment we use data sets that have at least two consecutive releases. 
\begin{itemize}
\item To generate recommendations for a release $i$, the planner uses releases releases $(i-1)$ and $(i-2)$.
\item The predictor also uses releases $(i-1)$ and $(i-2)$. However, we use SMOTE with re-sampling in order to handle the class imbalance in the data and to prevent the predictor from using the same training data as the planner.
\end{itemize}


The performance prediction data set was obtained from \cite{}. The data set contains six examples of real world configurable systems: \keyword{Apache}, \keyword{LLVM}, \keyword{x264}, \keyword{Berkeley DB (written in C and Java}, and \keyword{SQLite}. The systems have different characteristics, different implementation languages, and different configuration mechanisms. The data set is a collection of all possible configurations (with SQLITE being a exception with only 4653 configurations). We performed a 5-fold cross validation study on this data set.

\subsection{Performance Assessment}

In order to assess the performance of the planner for the defect data set, we use the Cliffs Delta score to measure the probability that the number of bugs in test data before applying the planner is larger than after doing so. In other words, we use the delta score to measure the ability of the planner to effective reduce the number of defects. Given the untreated test instance labeled \textit{Before} of length \textit{M} and the treated instances labeled \textit{After}, the delta score is obtained as follows:
\begin{equation}
\delta = \frac{\#(Before>After) - \#(Before<After)}{M^2}
\end{equation}

In the context of our application, the $\delta$ attains a value of 1 if the number of defects after applying the treatments has reduced to zero, and 0 if there is no change.

For the performance prediction data set on the other hand, Cliff's delta measure is not directly applicable. We therefore assess the performance by measuring the gain, given by $\frac{Before}{After}$. The gain takes a value larger than 1 if the run times have been reduced, a value of 1 if there is no change, and a value less than 1 if run times have increased after applying the recommended changes. 

In addition to the above, we rank the different variants of the planning scheme to identify the best approach. We make use of the Scott-Knott procedure, recommended by \cite{} to compute the ranks. It works as follows: A list of treatments \textit{l} is sorted by the median score. The list \textit{l} is then split into sub-lists m, n in order to maximize the expected value of the differences in the observed performance before and after division. A statistical hypothesis test \textit{H} is applied on the splits \textit{m, n} to check if they are statistically different. If so, Skott-Knott then recurses on each division. In our paper, we use the A12 test and a non-parametric bootstrap sampling for hypothesis testing. 
% \section{Experimental Results}
% \section{Discussion}
% \section{Threats to validity}
% \section{Conclusion}
% \section*{Acknowledgements}
%\begin{thebibliography}{10}
%\bibitem{test}
%\end{thebibliography}
\end{document}
