

\begin{figure*} 
~\hrule~
\begin{minipage}{.45\linewidth}
\small

~\\

{\bf The Line class (as described in \tion{mm}:}
Note that this is the primary data structure of HOW.
Training data is converted into a set of {\tt Line}s,
each of which is defined by a $X,Y$ pair, where $X$
has better scores than $Y$.
 
\begin{python}[left]
class Line:
  "Lines connect a worse cluster Y to a better one X"
  all   = []    # where to store all the lines
  using = []    # what columns to use
  F     = 0.5   # magnitude control parameter
  B     = 50    # ratio of how many columns to displace

 def __init__(i,x,y):
    if x.score < y.score:
      x,y = y,x      # x must have best score
    i.c = dist(x,y) 
    i.obj = len(x) 
    Line.all += [i]  # remember this line


  def displace(i,z):
    for j,(xj,zj) in enumerate(zip(x,z)):
      if j in Line.using and not j == i.obj:  
        z[j] = zj + Line.F*( xj - zj)
    return z

  def dist(i,z):
    _,out = geometry(i.x,i.y,z)
    return out
\end{python}

~\colorrule{gray}~



{\bf Main loop, of the  HOW  instance-based planner:}
Testing data is {\em displaced}
towards   the $X$ point of the nearest {\tt Line}.
\begin{python}[left]
def HOW(training, testing):
  def train(data):
    Line.using = bPruning(data)
    clusters   = cluster(data,len(data)):
    for cluster in clusters:
      one = exemplar(cluster)
      two = exemplar(nearest(one, clusters))
      Line(one,two)
 
   def exemplar(cluster):
    "Apply the rules at end of section II.B"

     
   def bPruning(data,  b = Line.B):
    "Return the indexes of the 'b' best columns" 
    enough  = int(len(weights)*b/100)
    return featureWeighting(data)[:enough]

  def nearestLine(z):
    out,least=None,10000
    for line in Line.all:
      d = line.dist(z)
      if d < least: out,least = line,d
    return out
   
  #---------------------------------
  # begin main code for  'HOW'
  train(training)
  return [nearestLine(z).displace(z) for z in testing]
\end{python}

~\colorrule{gray}~

{\bf Code for geometry (as described in \fig{where}c):}

 
\begin{python}[left]
def dist(x,y):
   "See Aha et al. reference [22]. XXX"
 
   
def geometry(x, y, c, z): 
  a = dist(z,x)
  b = dist(z,y) 
  x= $(a^2 + c^2 - b^2)/(2c)$ 
  y=  $\sqrt{(a^2 - max(x,0))^2}$
  return x,y
  
def nearest(x,data):
  return furthest(x,data,best=1000,
                    better=lambda j,k:j < k)
 
def furthest(x,data, best= -1, 
                     better=lambda j,k:j > k):  
  out = None
  for one in data:
    d = dist(one,x)
    if gt(d,best): out, best = y, d
  return out
\end{python}
 \end{minipage}~~~~~~~~~\begin{minipage}{.45\linewidth} 
\small

~\\

{\bf Clustering (as described in \fig{where}a):}
\begin{python}[right]
def cluster(data, n,lvl=100):
  def splitAcross2Points(data): 
    tmp = random.choose(data)
    x = furthest(tmp, data)
    y = furthest(x, data) 
    c = dist(x,y)  
    if x.scores < y.scores:
      x,y = y,x 
    for one in data.members: 
      one.pos = geometry(x,y,c,one)
    data = sorted(data) # sorted by 'pos.x'
    return x, y, split(data)
  
  def split(data):   
    mid = len(data)/2; 
    return data[mid:], data[:mid]
    
  # --------------------------------------
  # begin main code for  'cluster'
  if lvl < 1: 
     return data # stop if out of levels
  leafs = [] # Empty Set
  x,y,left,right = splitAcross2Points(data) 
  if len(left) > $\sqrt{n}$:  
     leafs += cluster(left, n, lvl - 1)  
  if len(right) > $\sqrt{n}$:  
     leafs += cluster(right,n,  lvl - 1) 
  data.has = dict(x=x,y=y,
                 l=left,r=right,
                 leafs=leafs)
  return leafs
  

\end{python} 

~\colorrule{gray}~

{\bf Feature weighting  (as described in \fig{where}d):}

\begin{python}[right]
def featureWeighting(cols):
  "Returns col indexes, sorted by their weight."
  class Stats():  
    """Utility class. Handles incremental update of
       n, mean, standard dev."""
    def __init__(i,inits=[]):
      i.n = i.mu = i.m2 = 0.0
      map(i.__add__,inits)
    def sd(i) :  return (max(0.0,i.m2)/(i.n - 1))**0.5
    def __add__(i,x):
      i.n  += 1
      delta = x - i.mu
      i.mu += delta/(1.0*i.n)
      i.m2 += delta*(x - i.mu) 
    def __sub__(i,x):
      i.n  -= 1
      delta = x - i.mu
      i.mu -= delta/(1.0*i.n)
      i.m2 -= delta*(x - i.mu) 
      
  def divide(this,tiny=2):
    "Find the split that most reduces std dev."
    lhs, rhs = Stats(), Stats(x[1] for x in this)
    n, least, cut = rhs.n*1.0, rhs.sd(), None
    for j,x in enumerate(this):
      if lhs.n > tiny and rhs.n > tiny:
        tmp = lhs.n/n*lhs.sd() + rhs.n/n * rhs.sd()
        if tmp < least:
           cut,least = j,tmp
      rhs - x[1]
      lhs + x[1]
    return cut,least
    
  def recurse(this,cuts):
    cut,sd = divide(this)
    if cut:
      recurse(this[:cut],cuts)
      recurse(this[cut:],cuts)
    else:
      cuts += [(sd,len(this)]
    return cuts  
    
  def weight1(i,obj,col):
    "Returns weight of column 'i'."
    pairs = sorted([row[i],row[obj] for row in col])
    n     = length(col)
    w     = sum(v*n1/n for v,n1 in recurse(pairs,[]))
    return w,i
    
  #---------------------------------
  # begin main code for  'featureWeighting'
  obj = len(cols[0]) - 1 
  return map(lambda k:k[1],
             sorted(weight1(col,i,obj) 
                    for i,col in enumerate(cols)
                    if not i == obj))
\end{python}
\end{minipage}
~\hrule~
\caption{HOW (Python-style psuedo-code).
For brevity's sake, this code skips certain low-level details.
For a full working implementation, see https://github.com/ai-se/HOW1/src.
Functions shown in \textcolor{blue}{{\bf blue}} are defined somewhere in this figure.}\label{fig:howcode}   
\end{figure*}
