 \documentclass[conference]{IEEEtran}
\usepackage{subfig}
\usepackage{wrapfig}
 \usepackage{amsmath}
 \usepackage{url}
 \usepackage{pifont}
 %\usepackage{times}
\usepackage{rotating}
%\usepackage{balance} 
\usepackage{color, colortbl}
\usepackage{graphicx}
\usepackage{algorithmicx}
\usepackage[running]{lineno}
\usepackage{program}
\usepackage{cite}
\usepackage{alltt}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\textsection\ref{sec:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\definecolor{lightgray}{gray}{0.975}
\usepackage{fancyvrb}
\usepackage{stfloats}
\usepackage{multirow}
\usepackage{listings}
\usepackage{amsmath}  
\DeclareMathOperator*{\argmin}{arg\,min} 
\DeclareMathOperator*{\argmax}{arg\,max} 


\usepackage{color}
\newcommand{\colorrule}[1]{\begingroup\color{#1}\hrule\endgroup}

\definecolor{darkgreen}{rgb}{0,0.3,0}

\usepackage[table]{xcolor}
\definecolor{Gray}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.85}
\definecolor{Blue}{RGB}{0,29,193}
\newcommand{\G}{\cellcolor{green}}
\newcommand{\Y}{\cellcolor{yellow}}


\definecolor{MyDarkBlue}{rgb}{0,0.08,0.45} 
\newenvironment{changed}{\par\color{MyDarkBlue}}{\par}

\newcommand{\ADD}[1]{\textcolor{MyDarkBlue}{{\bf #1}}}
\newcommand{\addit}[1]{\begin{changed}\input{#1}\end{changed}}

\usepackage{color}
\usepackage{listings}
\usepackage{setspace}

\definecolor{Gray}{gray}{0.9}
\newcommand{\kw}[1]{\textit{#1}}
\newcommand{\quart}[4]{\begin{picture}(75,6)
  {\color{black}\put(#3,3){\circle*{2.5}}\put(#1,3){\line(1,0){#2}}}\end{picture}}
% New Commands

\definecolor{Code}{rgb}{0,0,0}
\definecolor{Decorators}{rgb}{0.5,0.5,0.5}
\definecolor{Numbers}{rgb}{0.5,0,0}
\definecolor{MatchingBrackets}{rgb}{0.25,0.5,0.5}
\definecolor{Keywords}{rgb}{0,0,1}
\definecolor{self}{rgb}{0,0,0}
\definecolor{Strings}{rgb}{0,0.63,0}
\definecolor{Comments}{rgb}{0,0.63,1}
\definecolor{Comments}{rgb}{0.5,0.5,0.5}
\definecolor{Backquotes}{rgb}{0,0,0}
\definecolor{Classname}{rgb}{0,0,0}
\definecolor{FunctionName}{rgb}{0,0,0}
\definecolor{Operators}{rgb}{0,0,0}
\definecolor{Background}{rgb}{1,1,1}

\lstnewenvironment{python}[1][]{
\lstset{
mathescape,
numbers=right,
numberstyle=\scriptsize,
stepnumber=1,
numbersep=0.5em,
xleftmargin=0em,
framextopmargin=2em,
framexbottommargin=2em,
showspaces=false,
showtabs=false,
showstringspaces=false,
tabsize=2,
% Basic
basicstyle=\ttfamily\scriptsize\setstretch{0.8},
backgroundcolor=\color{Background},
language=Python,
% Comments
commentstyle=\color{Comments}\slshape,
% Strings
stringstyle=\color{Strings},
morecomment=[s][\color{Strings}]{"""}{"""},
morecomment=[s][\color{Strings}]{'''}{'''},
% keywords
morekeywords={[1]import,from,class,def,for,while,if,is,in,elif,else,not,and,or,print,break,continue,return,True,False,None,access,as,,del,except,exec,finally,global,import,lambda,pass,print,raise,try,assert, dot, norm, zip, sorted},
keywordstyle={[1]\color{Code}\bfseries},
% additional keywords
morekeywords={[3]fastmap, Line,bPruning,clister,train,leafs,weightedFeatures, exemplar,nearestLine,dist,displace,geometry,splitAcross2Points,leaves,How,nearest,bPruning,Stats,divide,recurse,weight1,project, furthest, split, WHERE,clusterer, getContours, envied, fWeight, nearestContour, projection, mutate, HERE, knn},
keywordstyle={[3]\color{Keywords}\bfseries},
morekeywords={[2]@invari},
keywordstyle={[2]\color{Decorators}\slshape},
emph={self},
emphstyle={\color{self}\slshape},
firstnumber=last
%
}}{}
 
\title{Learning Actionable Analytics 
 (with applications for reducing defects and reducing runtimes)}
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
Rahul Krishna, Tim Menzies, Xipeng Shen\\
       \affaddr{Computer Science, NcState, USA}\\
       \{i.m.ralk, tim.menzies, xipengshen\}@gmail.com
% 3rd. author
 % use '\and' if you need 'another row' of author names
% 4th. author
\and
 Andrian Marcus\\
       \affaddr{Computer Science, UtDallas, USA}\\
       {amarcus@utdallas.edu} }


  \pagestyle{plain}
\begin{document}
  \maketitle
  
  % make the title area
  
   
  \begin{abstract}
 As business users demand more insightful
 analytics, data scientists need to change
 their tools. Instead of merely predicting 
 some result, they also need tools that generate ``plans'';
 i.e. specific suggestions on  how to change values in order to
 improve on some predicted value.
 
 This paper proposes one such planner. ``HOW'' is a 
 tool for proposing changes to independent
 variables in order to improve on 
 the predictions of the dependent variables. Unlike other approaches
 for learning optimizations to software, HOW does not require
 an underlying of the domain. Also, HOW's plans
 are designed to result in {\em minimal displacement}
 to current practice.
 
 This paper uses  HOW to learn methods
 to reduce defects and decrease program runtime.
 For the test data of this paper, the improvements generated by HERE can reduce
 defect counts as well as runtimes by up to
 50\% of their original values.
  \end{abstract}
  \begin{IEEEkeywords}
Defect prediction, configuration, prediction, planning, instance-based reasoning.
  \end{IEEEkeywords}
  
\section{Introduction}
Business  users   now demand   data mining tools
that  support a  business-level
interpretation of that data. For example,
at a  panel on software analytics at ICSE'12,
industrial practitioners lamented the state of the art in data mining
and software engineering~\cite{menzies12a}. Panelists commented that
``prediction is all well and good, but what about decision
making?''. That is, these panelists are more interested in the interpretations
and follow-up
that occurs {\em after} the mining, rather than just  the mining itself. For example:
\bi
\item Instead of just accepting  {\em predictions} on how many 
 software defects
to expect,  business users might now demand a {\em plan} to
reduce the likelihood of those defects;
\item Instead of just accepting {\em predictions} on the runtime
time of their software, business users might now demand
a {\em plan} to reduce that runtime.
\ei
In these two points, we say that a ``plan'' is a  set of proposed changes in order to better achieve some goal. 
Generating {\em plans} is a  very different task the standard
data mining tasks of {\em predicting} some discrete or numeric class.
Predictors
and classifiers only tell us what {\em is} while planners 
  tell us {\em what to change}. For the kind of business user
discussed above, planners are the preferred to mere prediction systems
since, as one gruff user once shouted at us:
\begin{quote}
{\em ``Don't tell me what \underline{is};
tell me what to \underline{change}.''}
\end{quote}
Generating plans is also differed to the recommender systems. 
discussed by Robillard and Walker~\cite{rob14}.  Recommendor systems
in software engineering
are less {\em directive} and more    {\em collaborative} than plan generators:
\bi
\item
Directive systems (such as those discussed in this paper) tell developers exactly what to do;
e.g. ``use these settings in a Makefile''. 
\item
On the other hand, the collaborative systems discussed by Robillard and Walker
are more suggestive guides that
help (e.g.) programmers focus on very small sets
with  very large libraries of code of documentation.
\ei
This paper presents HOW, a novel planner for learning actionable analytics.
In the experiments shown below, we see that HOW's plans can
significantly reduce the expected
value of:
\bi
\item The software defects found in  the object-oriented JAVA systems explored in 2010 by Jureczko et al.~\cite{jureczko10};
\item The runtime of the  software systems whose configurations
are explored in 2013 by Seigmund et al.~\cite{sven12};
\ei
In summary, this paper makes five contributions that are significantly different to
prior work:
\be
\item The HOW  instance-based planner of this paper has not been presented before.
\item The experiments of this paper show that HOW planner is useful for real-world tasks;
e.g. reducing the
expected values of the Jureczko software defects and the Seigmund software runtimes.
\item Apart from HOW, we  also offers a general framework for using  predictive system to build planners;
\item Further, we offer a  test to recognizes  when we should {\em not}  plan: specifically,
planning is
not recommended in domains with  poorly performing predictors;
\item Last, we  offer a general verification rig for assessing the value of plans generated within this rig.
\ee
The rest of this paper is structured as follows.
HOW's instance-based reasoning  makes its conclusions by
studying the space of examples. As discussed in \tion{mm},
this is a very different approach to
{\em model-based planners} that use the model
as sub-routine within a Monte Carlo simulation~\cite{me07f} or the evolutionary 
programming methods explored by the search-based software
engineering community~\cite{krall14,harman12dec}.  
 
 

\section{HOW: Motivation and Methods}\label{sec:mm}
 
 \subsection{Motivation: Why ``HOW''?}
HOW was inspired by two     business issues raised about the  GALE optimizer~\cite{krall14}. 
A well-known effect in machine learning (and, in computer vision
and compression~\cite{levina04}) is that the $n$ dimensions of a data set can be represented by a  much
lower-dimensional approximation. GALE finds those $m \ll n$ dimensions then mutates examples
towards the better end of each dimension $m_i$. Since it explores $m \ll n$ dimensions,
GALE terminates much faster than standard optimiser (e.g. for one large model, 4 minutes versus 7 hours~\cite{krall14}).
Better yet, GALE's rapdily generated  solutions are competitive (or better) than those found by 
slower optimizers.

When GALE executes, it searches for better examples of data
 by (a)~inputting an initial population of data; then (b)~making
 extensive mutation to that data. In practice, GALE makes
100s to 100,000s of mutations. Each of these mutations
is reassessed using some domain model.
For certain users,this approach raises some issues.  One such user commented:
\begin{quote}
ISSUE~\#1: {\em ``What if I only want the minimum useful change, not
the maximum possible?''}
\end{quote}
Note here that this business user is asking for solutions  that do not require extensive 
changes to their current  approaches. 

Later in that discussion, another business user asked:
\begin{quote}
ISSUE~\#2: {\em ``But what if I do not have a domain model of my business?''}
\end{quote}
This also is a legitimate issue.
Building and certifying a model  can  takes  long time, especially in software engineering. For  example, in previous work~\cite{me07f} we used models
developed by Boehm's group at the University of Southern California.
Some of those models took decades to develop and mature (from 1981~\cite{boehm81} to 2000~\cite{boehm00b}). 

Also, even when there is an existing model, they can require
constant  maintenance lest they become out-dated. Elsewhere, we have described our
extensions to the USC models to enable reasoning about agile software developments. 
It took many months to implement and certify those extensions~\cite{me09i,me09j}.
The problem of model maintenance is another
motivation to look for alternate methods that can be automatically updated whenever new data arrives.

Further, sometimes  it is easier to access instances of a model's behaviour than the model
itself. For example, in prior work with Martin  Feather from the Jet Propulsion 
Laboratory~\cite{fea02a},  our research partner could not share a
propriety model from within the NASA firewalls. However, they could share 
logs of the input/output of that model.
 
These two issues are not just problems for GALE, but also for every
other model-based optimization algorithm  that makes
extensive use of mutation  and domain models such as
NSGA-II, NSGA-III, SPEA2, IBEA, PSO, DE, MOEA/D, etc.~\cite{deb00a,zit02,zit04,%
deb14,Cui2005a,storn97,zhang07:TEC}. Accordingly, this paper seeks
an optimization methods that make minimal changes to instances,
while avoiding the problem of models that are missing, slow
to build, hard to maintain, or difficult to access.

\subsection{Methods: How to Build ``HOW''?}

A general model for {\em predictors} is as follows. 
During testing, each test instance gains a prediction 
by finding which region of the data it falls into.
During training, those regions are inferred from
divisions that ``chunk'' together regions with instances
with similar properties. These dividers can be categorical (as in the case of decision trees) or probabilistic (as in the
case of Naive Bayes) or learned via some distance minimization
process 
(as in the case of support vector machines) or generated
directly by some clustering algorithm. 


\begin{figure}[t!]
\small
~\hrule~

{\bf \fig{where}a: top down clustering:}

Data can be rapidly and recursively divided   as follows.
Find   two   distance instances,  $X,Y$
by picking any instance $W$ at random, then setting $X$ to its most
distant instance, then setting $Y$ to the instance most distant from
$X$~\cite{fastmap}
(this requires only $O(2N)$ comparisons
of $N$ instances).
Next, project each instance $Z$
onto a line that  runs between $X,Y$ using the cosine
rule of \fig{where}b. After that,  split the data at the median $x$ value of all instances and
recurses on each half  (stopping when
one half has less  than $\sqrt{N}$ of the original population). For more details. \fig{howcode}, line~76, 

~\hrule~
 
{\bf \fig{where}b: finding distances:}

In the \fig{where}a, to compute distances, we use
the Euclidean measure recommended for
instance-based reasoning by Aha et al.~\cite{aha91};
i.e. $\sqrt{\sum_iw_i(X_i-Y_i)^2}$ where $X_i,Y_i$
where values are  normalized 0..1 for the range min..max and 
$w_i$ is some importance weight given to attribute $i$.
Usually, $w_i=1$ but it can be adjusted using, say,
the feature weighting methods of \fig{where}d. 
 
 ~\hrule~
 
{\bf \fig{where}c: 3 point geometry:}
 
Let   $X,Y$ be two points joined by  a line  length $c$.
A third point, $Z$, has distances  $a=dist(Z,X)$ and
$b=dist(Z,Y)$ to $X,Y$, respectively.
According to the cosine rule,   $Z$ projects onto  $\overline{XY}$
at distance $x=(a^2 + c^2 - b^2)/(2c)$ from $X$.
Further, according to Pythagoras' theorem, $Z$ stands at a distance
$y = \sqrt{a^2 - x^2}$ from the line $\overline{XY}$. For more details, see  \fig{howcode}, line~59.

~\hrule~

{\bf \fig{where}d: feature weighting:}

HERE's feature weighting algorithm
comes from the CART regression tree learner~\cite{Breiman1984}.
It sorts independent variables
 according to how well they can reduce the variability
of a  numeric objective.
If we split some column $i$ of independent numeric data  into $N=n_1 + n_2 + ..$ splits,
then the expected
value of the objective value of the split  is $w_i = \sum_i v_in_i/N$
where $v$ is the standard deviation of the objective.
A recursive procedure can  find those divisions $n_1,n_2,...$ by (a)~sorting a numeric column,
then split at $\argmin_i w_i$, then recursing into each half.  For more details, see  \fig{howcode}, line~109.

~\hrule~
 
\caption{Sub-routines within HOW.}\label{fig:where}
\end{figure}

\input{code}



Those same regions can be used for instance-based 
{\em planning}-- but instead of leaving test instances in the regions
found during training, we {\em displace} them towards regions
with better objective scores.
To achieve this, 
during training, HOW clusters data using the procedure of \fig{where}a,
then draws {\tt Line}s between the centroids of  clusters $X,Y$ that are nearest
neighbors.
By convention, we say that the cluster containing $X$ has the  better objective scores. This means we can say that the {\tt Line} is a useful direction for displacing
data; i.e.  away from $Y$ and towards $X$ (for an implementation of such a {\tt Line} class, see \fig{howcode}, line~1).
 During testing,  HOW   finds the  nearest {\tt Line} to every test instance. That  test instance is   then displaced towards the better end
of that line; i.e. towards  $X$ (see \fig{howcode}, line 53).

An important design decision with HOW 
is how to determine the exact {\em direction} and {\em magnitude} of the
displacement.  Following on from the discussion around   ISSUE~\#1,
we note that a design goal of HOW is
{\em minimal displacements} of project data.
To implement minimal displacement:
\bi
\item
HOW sorts  the independent variables
using the procedure of  \fig{where}d. It then only
displaces the    best $B$\%  of that sort, while leaving the rest untouched
(see \fig{howcode}, line 38).
In  the experiments shown below,  we explore   $B \in \{25,50,100\}\%$.
\item
HOW also  limits displacement
to very small regions in the space of the training data (never more that the separation
of the two closest clusters).  HOW's clusterer divides  data into many   small clusters containing
$\sqrt{N}$ of the population (using the {\tt cluster} function, \fig{howcode}, line 76).
HOW   limits displacement to  \mbox{$0 \le F \le c$}, where
$c$ is the distance between the two closest clusters. 
In  the experiments shown below,  we explore
\mbox{$F\in \{0.25, 0.5, 0.75\}$}.
\ei
Turning now to the displacement {\em direction},
our experiments show that it useful to  carefully reflect on exactly where HOW should  push each test instance.
HOW replaces each cluster   with one {\em exemplar} that best
characterizes the data in that cluster. It then displaces the
test data towards that exemplar, as shown in \fig{howcode}, line 19. In summary, 
If  $Z_j$ is an independent variable within the
test instance $Z$ and  $X$ is the exemplar found at training time,
then we displace $Z$ as follows:
\[Z_j =  Z_j + F*(X_j - Z_j)\]
Here, $F$ is the  magnitude controller, discussed above.
The experiments of this work showed that different kinds of data require
different kinds of exemplars.
\bi
\item
For the Seigmund data,
the objective score is the runtime of the test suite of a compiled program.
For this kind of data with numeric objectives,
the exemplar  is the instance in a cluster
with  best objective score.
\item
For the Jureczko data, the objective score
is a boolean that is true if defects have been noted
for that class. 
For this kind of data with discrete objectives,
the data, 
has less differentiation between the objective scores
so we set the exemplar to  
an instance synthesized from the median values of the instances
in that cluster.
\ei 

  
  
\begin{figure}[!t]
\small 
\[
\begin{array}{r}
\mathrm{project}\\
\mathrm{data}
\end{array} 
\left\{\begin{array}{l}\mathit{train}
        \left\{\begin{array}{l}
                \mathrm{learn\;a\;}\mathrm{predictor\;}\mathrm{(e.g.\;via \;Random\;Forest)}\\
                \mathrm{learn\;a\;}\mathrm{planner\;}\mathrm{(e.g.\;via \; HOW)}
              \end{array}\right.
       \\
      ~\\
\mathit{test}  
    \left\{\begin{array}{l@{~}l}
           \mathit{before}& =\mathrm{predictor}(\mathit{test})\\
           \mathrm{\bf if\;}\mathit{before} & <  \mathit{satisfactory}\\
           \mathrm{\bf then}  & \mathrm{{\bf return}\; 0}\\~\\
           \mathrm{\bf else} &
           \left\{
            \begin{array}{l}
                \mathit{test'} = \mathrm{planner}(\mathit{test})\\
                \mathit{after} =\mathrm{predictor}(\mathit{test'})\\ 
                \mathrm{{\bf return}\;} \mathit{before},\mathit{after}
            \end{array}
          \right.
   \end{array}\right.
\end{array} \right. 
\]
 
\caption{How to use {\em predictors} for {\em planning}.}\label{fig:work}
\end{figure}



\subsection{How to Assess ``HOW''}\label{sect:assess}
In the experiments shown below,  HOW comments  on how to reduce
software defects by adjusting certain static code parameters of the code\footnote{Such parameters can be manually adjusted by programmers or (for large scale changes) automatically adjusted using code refactoring
tools, as shown by Binkely et al. in 2006~\cite{Binkley2006} and more recently by Mkaouer et al. in 2015~\cite{Mkaouer15}.}. How are we to verify that HOW's recommendations actually work? 

Some organizations have the resources to 
run repeated trials to assess the effect of different treatments.
For example, in one remarkable recent study, Bente et al. report results
were the same specification was developed into a product by four different organizations~\cite{Anda2009}. Given those kind of resources, it would be possible
to (say) take a code base, assign it to $N$ different teams, apply
HOW's plans the code used by  $N/2$ teams, then check in 12 months time
if those halves of the teams have fewer defects than the other.  

Very few industrial or research groups have access
to the kinds of resources using by Bente et al. (evidence: in the size years since the
publication of that work, we know of only one of two similar studies). Hence, it
we wish to verify HOW's plans, then some other framework is required that is less
resource intensive.
 




We propose using prediction systems (built by data mining) as a verification
tool for planners. In domains with data, from which data miners can build
predictors of adequate performance, then those predictors can assess the value
of the changes proposed by HOW.

As shown in \fig{work}, our proposal divides
project data  into two disjoint sets {\em train} and {\em test}
(so \mbox{{\em train} $\cup$ {\em test} $=\;\emptyset$}).
Next, from the train set, we build both a {\em planner} as well
as a {  predictor}. Our general framework does not   commit to any particular  choice
of { planner} or { predictor} but, for the purposes of this paper, 
our { planner}
will be HOW  and our
our { predictors} will be the Random Forest and Random Forest
Regressor taken from the SciKit
Learn toolkit~\cite{Pedregosa2012}. The Random Forests procedure is explained in \fig{rr}. 

\begin{figure}[t!]
\small
\begin{tabular}{|p{.95\linewidth}|}\hline
 Decision tree learners find the attribute whose value's
select for for sections of the data with least variation in the class
variable. Decision tree learners then recurse on each selection~\cite{breiman84}. 

A random forest is a set of decision trees, each built
from some random subset of the rows and columns. The conclusions
of the forest are a ``vote'' generated by all the trees~\cite{Breiman2001}. 

A standard Random Forest predicts for a discrete class variable while
a Random Forest Regressor generates numeric predictions.\\\hline
\end{tabular}
\caption{Random Forests and Random Forest Regressors.}\label{fig:rr}
\end{figure}


Turning now to the {\em test} data, this is passed to the { predictor}
to generate a baseline {\em before} performance score.
Note that if our { predictors} fail to perform adequately on the test data,
then we cannot trust them to comment on our plans. Accordingly,
if that performance is unsatisfactory, we abort.
Else, we (1)~apply the { planner} to alter the {\em test} data;
then (2)~apply the { predictor} to the altered data {\em test'};
then (3)~return data on the {\em before,after} predictions.




\begin{figure*}[!t]
\scriptsize
   \begin{center}
   \begin{minipage}{.46\linewidth}
    \begin{tabular}{r@{~}|l@{~}|r@{~}|l@{~}|r@{~}|r@{~}|} \cline{2-6}
   & \multicolumn{5}{c|}{ }\\ 
   
   & \multicolumn{5}{c|}{ Data set  properties}\\ 
   & \multicolumn{5}{c|}{  }\\ 
           & \multicolumn{2}{c|}{training}   & \multicolumn{3}{c|}{testing}      \\ \cline{2-6}
   data set      & versions           & instances & versions     & instances    & \% defective             \\ \hline
        jedit    & 3.2, 4.0, 4.1, 4.2 & 1257      & 4.3          & 492          & 2 \\
        ivy      & 1.1, 1.4           & 352       & 2.0          & 352          & 11 \\
        camel    & 1.0, 1.2, 1.4      & 1819      & 1.6          & 965          & 19 \\
        ant      & 1.3, 1.4, 1.5, 1.6 & 947       & 1.7          & 745          & 22 \\
        synapse  & 1.0, 1.1           & 379       & 1.2          & 256          & 34 \\
        velocity & 1.4, 1.5           & 410       & 1.6          & 229          & 34 \\
        lucene   & 2.0, 2.2           & 442       & 2.4          & 340          & 59 \\
        poi      & 1.5, 2, 2.5        & 936       & 3.0          & 442          & 64 \\
        xerces   & 1.0, 1.2, 1.3      & 1055      & 1.4          & 588          & 74  \\ 
        log4j    & 1.0, 1.1           & 244       & 1.2          & 205          & 92   \\
        xalan    & 2.4, 2.5, 2.6      & 2411      & 2.7          & 909          & 99  \\\hline 
        
        
    \end{tabular}\end{minipage}\begin{minipage}{.4\linewidth}
    \begin{tabular}{|rrr|rrr|rr|l} \cline{1-8}
      \multicolumn{8}{|c|}{  }\\
      \multicolumn{8}{|c|}{  Results from learning}\\
       \multicolumn{8}{|c|}{   }\\
   \multicolumn{3}{|c|}{untuned} & \multicolumn{3}{c|}{tuned} & \multicolumn{2}{c|}{change}\\
  \cline{1-8}
  
  pd & pf & good? & pd & pf & good? & pd & pf\\\cline{1-8}
  50 & 27 &   & 64 & 28 & y & 14 & 1&$\star$\\
  65 & 35 & y & 65 & 25 & y & 0 & -10&$\star$\\
  50 & 30 &   & 62 & 41 &   & 12 & 11\\
  66 & 21 & y & 63 & 18 & y & -3 & -3\\
  45 & 18 &   & 47 & 15 &   & 2 & -3\\
  79 & 61 &   & 77 & 61 &   & -2 & 0\\
  55 & 25 &   & 60 & 27 & y & 5 & 2\\
  51 & 32 &   & 61 & 12 & y & 10 & -20&$\star$\\
 30 & 28 &   & 39 & 33 &   & 9 & 5&$\times$\\
  32 & 6 &   & 30 & 9 &   & -2 & 3&$\times$\\
  41 & 5 &   & 48 & 0 &   & 7 & -5&$\times$\\
\hline 
\end{tabular}

\end{minipage}
\end{center}    
  
    \caption{Training and test {\em data set properties} for the Jureczko data sets,
    sorted in ascending order of \% defective examples.
    On the right-hand-side, we show the {\em results from learning}.
    Data is ``good'' if it has   recalls over 60\% and false alarms under 40\%
(and note that, after tuning, there are more ``good'' than before).
Data   marked with ``$\star$'' show large improvements in performance, after tuning.
Data   marked with ``$\times$'' are not ``good'' since their test suites  have so few undefective examples (less than 5\% of the total sample) that it becomes harder to find better data towards which we can displace test data.
}\label{fig:j}
\end{figure*}


\section{When Not to Plan: the Problem of Poor Predictors}

The clear advantage of the approach of \fig{work} is that it can be readily and automatically
applied to domains with historical data. That said, the obvious drawback with this approach
is that our assessment of the plans is only as good as the predictor. To 
mitigate for this {\em problem of poor predictors}, it is hence wise to demand certain standards for the {\em
satisfactory} threshold used in this \fig{work}. 
 

For example, this study's data from two types of domains: the Jureczko   object-oriented data  
and the Seigmund configuration data. 
For the Seigmund data, the plans generated by HOW advise on how  to change the   configurations settings within a Makefile
(in order to make resulting compiled
code  run faster). Here, performance can be measured in terms of  difference
between the predicted runtime $p$ of test case items and their actual runtimes $a$
using  $s= 100*(1- \frac{abs(a - p)}{a})$.
This paper  explores    six Seigmund configuration data sets:  Berkeley DB (Java and Windows versions), Apache, SQLite, LLVM, and
  x264\footnote{Available from the performance prediction section of PROMISE
  http://openscience.us/repo/performance-predict.}. 
  After splitting that data into equal sized train:test groups, a Random Forest
  Regressor trained on one half, then applied to the other, achieved nearly perfect scores of
%issue 24
\[s=\{99.9, 99.8, 99.4, 99.1, 96,1\}\]
That is, we can be very confident that the predictors from the Seigmund data can assess
HOW's plans.
(Aside: if the reader doubts that such high scores are achievable, we note that these scores are 
consistent with those achieved by predictors built by Seigmund et al.~\cite{sven12}.)


\begin{figure*}[htbp!]
  \renewcommand{\baselinestretch}{0.8}\begin{center}
    {\scriptsize
      \begin{tabular}{c|l|p{4.7in}}
        amc & average method complexity & e.g. number of JAVA byte codes\\\hline
        avg\, cc & average McCabe & average McCabe's cyclomatic complexity seen
        in class\\\hline
        ca & afferent couplings & how many other classes use the specific
        class. \\\hline
class. \\\hline
        cam & cohesion amongst classes & summation of number of different
        types of method parameters in every method divided by a multiplication
        of number of different method parameter types in whole class and
        number of methods. \\\hline
        cbm &coupling between methods &  total number of new/redefined methods
        to which all the inherited methods are coupled\\\hline
        cbo & coupling between objects & increased when the methods of one
        class access services of another.\\\hline
        ce & efferent couplings & how many other classes is used by the
        specific class. \\\hline
        dam & data access & ratio of the number of private (protected)
        attributes to the total number of attributes\\\hline
        dit & depth of inheritance tree &\\\hline
        ic & inheritance coupling &  number of parent classes to which a given
        class is coupled (includes counts of methods and variables inherited)
        \\\hline
        lcom & lack of cohesion in methods &number of pairs of methods that do
        not share a reference to an instance variable.\\\hline
        locm3 & another lack of cohesion measure & if $m,a$ are  the number of
        $methods,attributes$
        in a class number and $\mu(a)$  is the number of methods accessing an
        attribute, 
        then
        $lcom3=((\frac{1}{a} \sum, j^a \mu(a, j)) - m)/ (1-m)$.
        \\\hline
        loc & lines of code &\\\hline
        max\, cc & maximum McCabe & maximum McCabe's cyclomatic complexity seen
        in class\\\hline
        mfa & functional abstraction & number of methods inherited by a class
        plus number of methods accessible by member methods of the
        class\\\hline
        moa &  aggregation &  count of the number of data declarations (class
        fields) whose types are user defined classes\\\hline
        noc &  number of children &\\\hline
        npm & number of public methods & \\\hline
        rfc & response for a class &number of  methods invoked in response to
        a message to the object.\\\hline
        wmc & weighted methods per class &\\\hline
        \rowcolor{lightgray}
        defect & defect & Boolean: where defects found in post-release bug-tracking systems.
      \end{tabular}
    }
  \end{center}
  \caption{OO measures used in our defect data sets.  Last line is
    the dependent attribute (whether a defect is reported to  a
    post-release bug-tracking system).}\label{fig:ck}
\end{figure*}

Turning now to the Jureczko   OO data sets, we found:
\bi
\item
Most   predictors in this domain were far from      perfect;
\item 
But several of these predictors could
be salvaged using some sampling and optimization methods (see the notes on SMOTE and differential evolution, below).
\ei
As shown in \fig{j}, we   explore numerous examples of the Jureczko   object-oriented static code data sets: Ant, Camel, Ivy, Jedit,   Log4, Lucene, Poi, Synapse, Velocity, Xalan, Xerces\footnote{Available from the object-oriented defects section of the PROMISE respository openscience.us/repo/defect/ck.}. 
Given access to $V$ released
versions, we test on version $V$ and train on the available data from earlier releases (as
shown in \fig{j}, this means that we are training on hundreds to thousands
of classes and testing on smaller test suites).
Note the three bottom rows marked with $\times$: these contain predominately
defective classes (two-thirds, or more) and in such data sets, it is hard to distinguish
good from bad (since there are so many bad examples). 

For the  Jureczko   data,
the plans generated by HOW advise   how to change static code attributes in order to reduce defects in
Java classes.  Here,
performance can be measured using:
\bi
\item Probability of detection (a.k.a. ``pd'' or recall):  the percent of faulty classes in
the test data detected
by the {\em predictor}.
\item Probability of false alarm (a.k.a. ``pf''): the percent of non-fault
classes that are predicted to be defective.
\ei 
The ``untuned'' columns of \fig{j} shows RandomForest's learned {\em pd,pf}
values generated after learning from the training data, then being applied to the test data.
If we define ``good'' to mean $\mathit{pd}>60 \wedge \mathit{pf} < 40$\%,
then only two of our data sets ({\em ivy,ant}) are ``good'' enough. 

The ``tuned'' columns of \fig{j} show that we can salvage some of the data sets
using two techniques.
 Pelayo and Dick~\cite{pelayo07} report that defect prediction can be improved via SMOTE~\cite{Chawla2002}; i.e. an over-sampling of minority-class examples.
 Also, Fu et al.~\cite{fu:ase15} report that an optimization method called differential evolution~\cite{storn97}
applied to defect predictors can quickly explore the tuning
options of Random Forest to find better settings for the size of the forest, the termination criteria
for tree generation, etc.
The rows marked with $\star$ in \fig{j} show data sets whose performance was improved remarkably by these
techniques. For example, in {\em poi}, the recall increased by 10\% while the false alarm rate dropped by 20\%.
However,  as might be expected
the data sets in the  three bottom rows remain
unsalvageable.

In summary, while we cannot trust predictors from some of our defect data sets,
we can plan ways to reduce defects in {\em jedit, ivy, ant, lucene} and {\em poi}.
(Aside: note that this tuning used optimization and sub-sampling   on   {\em only} the
training instances of \fig{j}
i.e.
no clues from the test set were ever used in this tuning process.)


 

\section{Results}

\subsection{Data}
To assess HOW, we applied its plans to all the Seigmund data, and the five ``good''
  Jureczko   data sets {\em jedit, ivy, ant, lucene} and {\em poi}.
  The training and tests used for Jurecko were display in \fig{j}. For these data
  sets, training was managed by a Random Forest (tuned in the SMOTED training data using differential
  evolution). \fig{ck} shows the attributes in the Jureczko data.
  
  The Seigmund data sets are far more varied than the Jurecko data (since they contain every configuration option in the Makefiles
  for BerkeleyDB-- the Java  and  Windows  versions,  Apache,  SQLite,  LLVM,and x264).
  \fig{cpm} lists the size of Seigmund attribute sets. For the Siegmind data, 
  25 times, 
  we randomized the order of the data, trained on the first half (using
  Random Forest Regressor), then tested on the second half.
  
  
  

 

\begin{figure}[!hbp]
  \renewcommand{\baselinestretch}{1}\begin{center}
{\scriptsize
\begin{tabular}{llllll}
  \hline
  \rowcolor{lightgray}
Project & Domain & Lang. & LOC & Features & Config\\\hline

Berkeley DB CE & Database & C & 219,811 & 18 & 2560\\

Berkeley DB JE & Database & Java & 42,596 & 32  & 400\\

Apache & Web Server & C & 230,277 & 9 & 192\\

SQLite & Database & C & 312,625 & 39 & 3,932,160\textsuperscript{*}\\

LLVM & Compiler & C++ & 47,549 & 11 & 1024\\

x264 & Video Enc. & C& 45,743 & 16 & 1152\\\hline
\end{tabular}}\par\medskip

{ \footnotesize $^*$ Dataset contains 4, 553 configurations for prediction modeling and 100 additional random configurations for prediction evaluation, see \cite{vapp}.}
\end{center}

\caption{Seigmund data sets: details on the data sets}\label{fig:cpm}
\end{figure}



%\begin{figure}[tbp!]
% \subfloat[\label{oracle}]{\includegraphics[width=\linewidth]{{_figs//Oracle}}\\
%\includegraphics[width=\linewidth]{_figs/WHAT}
%\caption{Experimental Rig}
%\label{fig:rig}
  
  
%   \subfloat[Experimental Rig for Defect Prediction]{  \includegraphics[width=\linewidth]{_figs/HERE}
%   \label{fig:rig2}}

%\end{figure}

%involves two key components. Firstly, there is the oracle (Random Forest with SMOTE) which is used to make predictions. Then we have the 


\begin{figure}[tbp]
\centering
\includegraphics[width=\linewidth]{_figs/histograms.pdf}
\caption{Jureczko object-oriented defect data sets. 
Comparisons of predictions of defects in the Jureczko data sets before and after planning.}
\label{fig:bugs}
\end{figure}

\subsection{Visualisations of Changes}

In this section, we show some sample results. In the next section, we conduct a statistical
analysis of all   results.

\fig{bugs} shows the effects of planning on the five good Jureczko data sets. Note that:
\bi
\item
The number of defects predicted in the ``after'' set is always less than ``before'';
\item
The reduction can be quite large: e.g. see the over 50\% reduction in the {\em ant} data set.
\ei

 \begin{figure*}[htbp!]
\centering
\begin{minipage}{0.30\linewidth}
\includegraphics[width=\linewidth]{_figs/Apache.pdf}
\end{minipage}
\begin{minipage}{0.30\linewidth}
\includegraphics[width=\linewidth]{_figs/BDBJ.pdf}
\end{minipage}
\begin{minipage}{0.30\linewidth}
\includegraphics[width=\linewidth]{_figs/BDBC.pdf}
\end{minipage}\\[0.25cm]

\begin{minipage}{0.30\linewidth}
\includegraphics[width=\linewidth]{_figs/LLVM.pdf}
\end{minipage}
\begin{minipage}{0.30\linewidth}
\includegraphics[width=\linewidth]{_figs/SQL.pdf}
\end{minipage}
\begin{minipage}{0.30\linewidth}
\includegraphics[width=\linewidth]{_figs/X264.pdf}
\end{minipage}
\caption{Run time optimization using HERE . Blue curve represents a mutation probability of 0.75 with feature weighting and information pruning; Red curve represents a mutation probability of 0.25 with feature weighting, but no information pruning; Black curve represents the raw test data used as baseline for the comparison.}
\label{fig:pp}
\end{figure*}

\fig{pp} shows the effects of planning on the Seigmund data sets. The black, red %XXX
curves show the
runtimes as predicted in the test sets before/after our planning.  While some of the differences
are small (eg. LLVM and SQL), some of the others are very large indeed (e.g. the Apache and   BD*).
 
This visual samples of HOW's planning results look promising-- but they need to be confirmed by a
 more rigorous statistical analysis.
 
\subsection{Statistical Analysis}

HOW's performance can be adjusted in numerous ways:
\bi
\item The $F$ variable controls the size of the displacement;
\item The $B$ variable controls how many of the attributes are displaced;
\item The $w_i$ weight inside our distance calcylation controls how important are each variable in computing
     the space between instances. A standard default is $w_i=1$ but this can be adjusted via
     the {\tt featureWeighting} procedure of \fig{howcode}.
\ei

To understand how much (if at all) these variables change the planning results, 
this study ranks methods using the Scott-Knott
procedure recommended by Mittas \& Angelis in their 2013
IEEE TSE paper~\cite{mittas13}.  This method
sorts a list of $l$ treatments with $ls$ measurements by their median
score. It then
splits $l$ into sub-lists $m,n$ in order to maximize the expected value of
 differences  in the observed performances
before and after divisions. E.g. for lists $l,m,n$ of size $ls,ms,ns$ where $l=m\cup n$:
 $E(\Delta)=\frac{ms}{ls}abs(m.\mu - l.\mu)^2 + \frac{ns}{ls}abs(n.\mu - l.\mu)^2$.
Scott-Knott then applies some statistical hypothesis test $H$ to check
if $m,n$ are significantly different. If so, Scott-Knott then recurses on each division.
 
For this study, our hypothesis test $H$ was a
conjunction of the A12 effect size test of  a
non-parametric bootstrap sampling; i.e. our
Scott-Knott divided the data if {\em both}
bootstrapping and an effect size test agreed that
the division was statistically significant (99\%
confidence) and not a ``small'' effect ($A12 \ge
0.6$).
For a justification of the use of non-parametric
bootstrapping, see Efron \&
Tibshirani~\cite[p220-223]{efron93}.
For a justification of the use of effect size tests
see Shepperd\&MacDonell~\cite{shepperd12a}; Kampenes~\cite{kampenes07}; and
Kocaguenli et al.~\cite{kocharm13}. These researchers
warn that even if an
hypothesis test declares two populations to be
``significantly'' different, then that result is
misleading if the ``effect size'' is very small.
Hence, to assess 
the performance differences 
we first must rule out small effects.
This test was recently 
endorsed by Arcuri and Briand
at ICSE'11~\cite{arcuri11}.

 
  



To get an overall count of the   in  order to avoid any parametric bias,
we use the Cliffs Delta score to measure the probability that performance has been changed
by the planner.   Given the untreated test data labeled \textit{Before} of length \textit{M} and the treated instances labeled \textit{After}, the delta score is obtained as follows:
\begin{equation}
\delta = \frac{\#(Before<After) - \#(Before>After)}{M^2}
\label{eq:cliffs}
\end{equation}

A $\delta$ of  0 implies no change at all and, according to Romano et al.~\cite{romano06}, a value less
that 14]7\% is a ``small'' (i.e. negliable) effect.

\begin{figure*}[htbp!]

\noindent\begin{minipage}{0.5\textwidth}
  {\scriptsize \textbf{Ant}\\}
  {\scriptsize  \begin{tabular}{{l@{~~~~}l@{}r@{~~~~}r@{~~~~}c}}
      \arrayrulecolor{darkgray}
      \rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 & 0.75 &    -0.63  &  0.08 & \quart{2}{7}{5}{144} \\
  1 & 0.75, weighting &    -0.6  &  0.2 & \quart{0}{16}{8}{144} \\
\hline  2 & 0.5, weighting &    -0.42  &  0.11 & \quart{17}{10}{23}{144} \\
  2 & 0.5 &    -0.42  &  0.11 & \quart{19}{9}{23}{144} \\
  2 & 0.75, weighting, Prune=50\% &    -0.4  &  0.11 & \quart{20}{9}{25}{144} \\
\hline  3 & 0.25 &    -0.28  &  0.09 & \quart{31}{7}{35}{144} \\
  3 & 0.25, weighting &    -0.28  &  0.08 & \quart{31}{7}{35}{144} \\
\hline  4 & 0.5, weighting, Prune=50\% &    -0.25  &  0.06 & \quart{35}{5}{38}{144} \\
\hline  5 & 0.25, weighting, Prune=50\% &    -0.19  &  0.05 & \quart{41}{4}{43}{144} \\
\hline  6 & Baseline &    -0.11  &  0.03 & \quart{47}{2}{49}{144} \\
\hline \end{tabular}}
\end{minipage}
\noindent\begin{minipage}{0.5\textwidth}
  \flushleft
  {\scriptsize \textbf{Camel}\\}
{\scriptsize  \begin{tabular}{{l@{~~~~}l@{}r@{~~~~}r@{~~~~}c}}
\arrayrulecolor{darkgray}
\rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 &  0.75, weighting, Prune=50\% &    -0.41  &  0.1 & \quart{2}{14}{9}{205} \\
  1 &  0.75 &    -0.41  &  0.23 & \quart{1}{32}{9}{205} \\
  1 &  0.75, weighting &    -0.39  &  0.16 & \quart{0}{22}{12}{205} \\
\hline  2 &  0.5, weighting, Prune=50\% &    -0.27  &  0.09 & \quart{19}{12}{29}{205} \\
  2 &  0.5 &    -0.26  &  0.12 & \quart{20}{17}{30}{205} \\
  2 &  0.5, weighting &    -0.24  &  0.09 & \quart{24}{13}{33}{205} \\
\hline  3 &  0.25, weighting, Prune=50\% &    -0.2  &  0.05 & \quart{36}{7}{38}{205} \\
  3 &  0.25, weighting &    -0.19  &  0.07 & \quart{36}{9}{40}{205} \\
  3 &  0.25 &    -0.17  &  0.06 & \quart{37}{8}{43}{205} \\
\hline  4 &  Baseline &    -0.15  &  0.06 & \quart{41}{8}{45}{205} \\
\hline \end{tabular}}
\end{minipage}

\noindent\begin{minipage}{0.5\textwidth}
  \flushleft
  {\scriptsize \textbf{Ivy}\\[-0.05cm]}
{\scriptsize  \begin{tabular}{{l@{~~~~}l@{}r@{~~~~}r@{~~~~}c}}
\arrayrulecolor{darkgray}
\rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 &     0.75 &    -0.57  &  0.17 & \quart{0}{16}{6}{163} \\
  1 &   0.75, weighting &    -0.55  &  0.11 & \quart{3}{11}{8}{163} \\
\hline  2 &   0.75, weighting, Prune=50\% &    -0.38  &  0.11 & \quart{19}{11}{25}{163} \\
  2 &    0.5, weighting &    -0.36  &  0.12 & \quart{23}{12}{27}{163} \\
  2 &   0.5 &    -0.33  &  0.11 & \quart{25}{11}{30}{163} \\
\hline  3 &   0.5, weighting, Prune=50\% &    -0.27  &  0.08 & \quart{30}{8}{36}{163} \\
\hline  4 &   0.25, weighting &    -0.2  &  0.07 & \quart{41}{7}{43}{163} \\
  4 &     0.25 &    -0.16  &  0.04 & \quart{43}{4}{47}{163} \\
  4 &     Baseline &    -0.16  &  0.0 & \quart{47}{0}{47}{163} \\
\hline  5 &   0.25, weighting, Prune=50\% &    -0.15  &  0.04 & \quart{45}{4}{48}{163} \\
\hline \end{tabular}}
\end{minipage}
\noindent\begin{minipage}{0.5\textwidth}
  \flushleft
  {\scriptsize \textbf{Jedit}\\}
  {\scriptsize  \begin{tabular}{{l@{~~~~}l@{}r@{~~~~}r@{~~~~}c}}
\arrayrulecolor{darkgray}
\rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 &  0.75, weighting, Prune=50\% &    -0.41  &  0.18 & \quart{0}{20}{8}{172} \\
  1 &  0.75 &    -0.35  &  0.26 & \quart{0}{30}{15}{172} \\
\hline  2 &  0.75, weighting &    -0.25  &  0.17 & \quart{16}{20}{26}{172} \\
  2 &  0.5, weighting, Prune=50\% &    -0.21  &  0.1 & \quart{23}{11}{31}{172} \\
\hline  3 &   0.5 &    -0.17  &  0.08 & \quart{32}{9}{36}{172} \\
  3 &  0.5, weighting &    -0.14  &  0.09 & \quart{31}{10}{39}{172} \\
\hline  4 &  0.25, weighting, Prune=50\% &    -0.12  &  0.06 & \quart{39}{7}{41}{172} \\
\hline  5 &  0.25 &    -0.08  &  0.05 & \quart{43}{5}{46}{172} \\
  5 &  0.25, weighting &    -0.08  &  0.06 & \quart{43}{6}{46}{172} \\
\hline  6 &  Baseline &    -0.06  &  0.01 & \quart{47}{1}{48}{172} \\
\hline \end{tabular}}
\end{minipage}


\noindent\begin{minipage}{0.5\textwidth}
  \flushleft
  {\scriptsize \textbf{Log4j}\\}
  {\scriptsize  \begin{tabular}{{l@{~~~~}l@{}r@{~~~~}r@{~~~~}c}}
\arrayrulecolor{darkgray}
\rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 &  0.75, weighting, Prune=50\% &    -0.19  &  0.13 & \quart{0}{28}{15}{273} \\
\hline  2 &  0.75 &    -0.13  &  0.06 & \quart{23}{13}{28}{273} \\
  2 &  0.5, weighting, Prune=50\% &    -0.13  &  0.05 & \quart{23}{11}{28}{273} \\
  2 &  0.75, weighting &    -0.12  &  0.1 & \quart{15}{21}{30}{273} \\
\hline  3 &  0.5, weighting &    -0.1  &  0.03 & \quart{28}{6}{34}{273} \\
\hline  4 &   0.5 &    -0.1  &  0.07 & \quart{28}{15}{34}{273} \\
  4 &  0.25 &    -0.09  &  0.04 & \quart{34}{9}{36}{273} \\
  4 &  0.25, weighting, Prune=50\% &    -0.09  &  0.04 & \quart{34}{9}{36}{273} \\
  4 &  0.25, weighting &    -0.06  &  0.04 & \quart{34}{9}{43}{273} \\
\hline  5 &  Baseline &    -0.06  &  0.03 & \quart{43}{6}{43}{273} \\
\hline \end{tabular}}
\end{minipage}
\noindent\begin{minipage}{0.5\textwidth}
  \flushleft
  {\scriptsize \textbf{Lucene}\\}
  {\scriptsize  \begin{tabular}{{l@{~~~~}l@{}r@{~~~~}r@{~~~~}c}}
\arrayrulecolor{darkgray}
\rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 &  0.75, weighting &    -0.15  &  0.09 & \quart{0}{29}{9}{393} \\
  1 &  0.75 &    -0.13  &  0.09 & \quart{0}{29}{16}{393} \\
  1 &  0.75, weighting, Prune=50\% &    -0.12  &  0.08 & \quart{6}{27}{19}{393} \\
\hline  2 &  Baseline &    -0.1  &  0.03 & \quart{26}{10}{26}{393} \\
  2 &  0.5, weighting, Prune=50\% &    -0.1  &  0.03 & \quart{23}{10}{26}{393} \\
\hline  3 &  0.5, weighting &    -0.08  &  0.06 & \quart{23}{20}{33}{393} \\
  3 &  0.25, weighting, Prune=50\% &    -0.07  &  0.03 & \quart{33}{10}{36}{393} \\
  3 &  0.5 &    -0.07  &  0.05 & \quart{29}{17}{36}{393} \\
\hline  4 &  0.25, weighting &    -0.04  &  0.04 & \quart{36}{13}{46}{393} \\
  4 &  0.25 &    -0.04  &  0.02 & \quart{39}{7}{46}{393} \\
\hline \end{tabular}}
\end{minipage}

\noindent\begin{minipage}{0.5\textwidth}
  \flushleft
  {\scriptsize \textbf{Poi}\\}
  {\scriptsize  \begin{tabular}{{l@{~~~~}l@{}r@{~~~~}r@{~~~~}c}}
      \arrayrulecolor{darkgray}
\rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 & 0.75, weighting &    -0.45  &  0.1 & \quart{6}{10}{11}{162} \\
  1 &  0.75 &    -0.44  &  0.23 & \quart{0}{23}{12}{162} \\
\hline  2 & 0.5, weighting &    -0.32  &  0.23 & \quart{12}{24}{24}{162} \\
  2 & 0.5 &    -0.26  &  0.23 & \quart{18}{24}{31}{162} \\
  2 & 0.75, weighting, Prune=50\% &    -0.25  &  0.19 & \quart{17}{20}{32}{162} \\
\hline  3 & 0.5, weighting, Prune=50\% &    -0.19  &  0.09 & \quart{34}{9}{38}{162} \\
  3 &  0.25 &    -0.19  &  0.11 & \quart{32}{11}{38}{162} \\
  3 & 0.25, weighting &    -0.17  &  0.14 & \quart{30}{14}{40}{162} \\
\hline  4 & 0.25, weighting, Prune=50\% &    -0.12  &  0.09 & \quart{40}{9}{45}{162} \\
\hline  5 &  Baseline &    -0.09  &  0.03 & \quart{46}{3}{48}{162} \\
\hline \end{tabular}}
\end{minipage}
\noindent\begin{minipage}{0.5\textwidth}
  \flushleft
  {\scriptsize \textbf{Synapse}\\}
  {\scriptsize  \begin{tabular}{{l@{~~~~}l@{}r@{~~~~}r@{~~~~}c}}
\arrayrulecolor{darkgray}
\rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 & 0.75, weighting &    -0.56  &  0.2 & \quart{7}{18}{11}{148} \\
  1 & 0.75 &    -0.48  &  0.34 & \quart{0}{29}{18}{148} \\
\hline  2 & 0.5 &    -0.4  &  0.27 & \quart{14}{24}{25}{148} \\
  2 & 0.5, weighting &    -0.37  &  0.17 & \quart{21}{15}{28}{148} \\
\hline  3 & 0.25 &    -0.23  &  0.16 & \quart{31}{14}{40}{148} \\
  3 & 0.25, weighting &    -0.23  &  0.1 & \quart{38}{9}{40}{148} \\
  3 & 0.75, weighting, Prune=50\% &    -0.19  &  0.08 & \quart{40}{7}{43}{148} \\
\hline  4 & 0.25, weighting, Prune=50\% &    -0.17  &  0.08 & \quart{42}{7}{45}{148} \\
  4 & Baseline &    -0.15  &  0.0 & \quart{47}{0}{47}{148} \\
  4 & 0.5, weighting, Prune=50\% &    -0.15  &  0.11 & \quart{40}{9}{47}{148} \\
\hline \end{tabular}}
\end{minipage}

\noindent\begin{minipage}{0.50\textwidth}
  \flushleft
  {\scriptsize \textbf{Velocity}\\}
  {\scriptsize  \begin{tabular}{{l@{~~~~}l@{}r@{~~~~}r@{~~~~}c}}
\arrayrulecolor{darkgray}
\rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 & 0.75, weighting &    -0.19  &  0.22 & \quart{1}{34}{21}{207} \\
  1 & 0.75, weighting, Prune=50\% &    -0.15  &  0.13 & \quart{14}{20}{28}{207} \\
  1 & 0.75 &    -0.14  &  0.23 & \quart{0}{35}{29}{207} \\
\hline  2 & 0.5 &    -0.13  &  0.11 & \quart{23}{17}{31}{207} \\
\hline  3 & 0.5, weighting &    -0.1  &  0.08 & \quart{29}{13}{35}{207} \\
  3 & 0.5, weighting, Prune=50\% &    -0.07  &  0.07 & \quart{32}{11}{40}{207} \\
\hline  4 & Baseline &    -0.05  &  0.04 & \quart{43}{6}{43}{207} \\
  4 & 0.25, weighting &    -0.05  &  0.03 & \quart{40}{5}{43}{207} \\
  4 & 0.25 &    -0.04  &  0.02 & \quart{43}{3}{45}{207} \\
  4 & 0.25, weighting, Prune=50\% &    -0.01  &  0.04 & \quart{43}{6}{49}{207} \\
\hline \end{tabular}}
\end{minipage}
\noindent\begin{minipage}{0.50\textwidth}
  \flushleft
  {\scriptsize \textbf{Xalan}\\}
  {\scriptsize  \begin{tabular}{{l@{~~~~}l@{}r@{~~~~}r@{~~~~}c}}
\arrayrulecolor{darkgray}
\rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 & 0.75 &    -0.36  &  0.12 & \quart{0}{16}{9}{198} \\
  1 & 0.75, weighting &    -0.3  &  0.17 & \quart{4}{23}{18}{198} \\
  1 &    0.5 &    -0.3  &  0.12 & \quart{9}{17}{18}{198} \\
\hline  2 & 0.5, weighting &    -0.28  &  0.09 & \quart{16}{13}{20}{198} \\
\hline  3 & 0.25, weighting &    -0.23  &  0.1 & \quart{20}{14}{27}{198} \\
  3 & 0.25 &    -0.23  &  0.12 & \quart{20}{17}{27}{198} \\
\hline  4 & 0.25, weighting, Prune=50\% &    -0.14  &  0.07 & \quart{36}{9}{40}{198} \\
  4 & 0.75, weighting, Prune=50\% &    -0.14  &  0.07 & \quart{33}{10}{40}{198} \\
  4 & 0.5, weighting, Prune=50\% &    -0.12  &  0.06 & \quart{37}{8}{43}{198} \\
\hline  5 & Baseline &    -0.08  &  0.02 & \quart{47}{2}{48}{198} \\
\hline \end{tabular}}
\end{minipage}
\caption{Cliffs Delta Scores for the defect data set. The Treatment column, has 3 possible configurations, the first (0.25, 0.50, and 0.75) represents the extent of mutation; weighting represents feature weighting; Prune represents the extent of information pruning.}
\label{tab:ant}
\end{figure*}





\subsection{Performance Prediction Data set}

In this section, 
\begin{figure*}[htbp!]
\noindent\begin{minipage}{0.5\textwidth}
  {\scriptsize \textbf{Apache}\\}
  {\scriptsize  \begin{tabular}{{l@{~~~~}l@{~~~~}r@{~~~~}r@{~~~~}c}}
      \arrayrulecolor{darkgray}
      \rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 & 75, weighting, Prune=25\% &    0.77  &  0.05 & \quart{0}{9}{3}{49} \\
  1 &   75 &    0.79  &  0.08 & \quart{1}{16}{7}{49} \\
\hline  2 & 75, weighting &    0.85  &  0.07 & \quart{9}{14}{19}{49} \\
  2 & 50, weighting, Prune=25\% &    0.85  &  0.04 & \quart{15}{8}{19}{49} \\
  2 &  50 &    0.85  &  0.05 & \quart{17}{10}{19}{49} \\
\hline  3 & 50, weighting &    0.91  &  0.05 & \quart{25}{10}{31}{49} \\
  3 &  25 &    0.92  &  0.04 & \quart{29}{8}{33}{49} \\
  3 & 25, weighting, Prune=25\% &    0.93  &  0.02 & \quart{33}{4}{35}{49} \\
\hline  4 & 25, weighting &    0.96  &  0.03 & \quart{37}{6}{41}{49} \\
\hline  5 & baseline &    1.0  &  0.0 & \quart{49}{0}{49}{49} \\
\hline \end{tabular}}
\end{minipage}
\noindent\begin{minipage}{0.5\textwidth}
  \flushleft
  {\scriptsize \textbf{BDBC}\\}
{\scriptsize  \begin{tabular}{{l@{~~~~}l@{~~~~}r@{~~~~}r@{~~~~}c}}
\arrayrulecolor{darkgray}
\rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 &     75 &    0.22  &  0.05 & \quart{0}{3}{1}{49} \\
  1 &  75, weighting, Prune=25\% &    0.21  &  0.04 & \quart{0}{2}{0}{49} \\
\hline  2 &  75, weighting &    0.24  &  0.1 & \quart{0}{6}{2}{49} \\
\hline  3 &     50 &    0.4  &  0.04 & \quart{11}{2}{12}{49} \\
  3 &  50, weighting, Prune=25\% &    0.4  &  0.05 & \quart{10}{3}{12}{49} \\
\hline  4 &  50, weighting &    0.47  &  0.09 & \quart{14}{5}{16}{49} \\
\hline  5 &     25 &    0.66  &  0.03 & \quart{28}{1}{28}{49} \\
  5 &  25, weighting, Prune=25\% &    0.67  &  0.06 & \quart{28}{3}{29}{49} \\
\hline  6 &  25, weighting &    0.69  &  0.06 & \quart{29}{4}{30}{49} \\
\hline  7 &  baseline &    1.0  &  0.0 & \quart{49}{0}{49}{49} \\
\hline \end{tabular}}
\end{minipage}

\noindent\begin{minipage}{0.5\textwidth}
  \flushleft
  {\scriptsize \textbf{BDBJ}\\}
{\scriptsize  \begin{tabular}{{l@{~~~~}l@{~~~~}r@{~~~~}r@{~~~~}c}}
\arrayrulecolor{darkgray}
\rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 &     75 &    0.63  &  0.16 & \quart{0}{19}{3}{49} \\
  1 &  75, weighting, Prune=25\% &    0.68  &  0.17 & \quart{1}{21}{9}{49} \\
  1 &  75, weighting &    0.74  &  0.11 & \quart{8}{14}{17}{49} \\
\hline  2 &  50, weighting, Prune=25\% &    0.75  &  0.09 & \quart{14}{12}{18}{49} \\
  2 &     50 &    0.79  &  0.13 & \quart{16}{16}{23}{49} \\
\hline  3 &  50, weighting &    0.82  &  0.09 & \quart{23}{11}{27}{49} \\
\hline  4 &  25, weighting, Prune=25\% &    0.87  &  0.07 & \quart{29}{9}{33}{49} \\
\hline  5 &     25 &    0.88  &  0.06 & \quart{33}{8}{34}{49} \\
  5 &  25, weighting &    0.92  &  0.06 & \quart{36}{7}{39}{49} \\
\hline  6 &  baseline &    1.0  &  0.0 & \quart{49}{0}{49}{49} \\
\hline \end{tabular}}
\end{minipage}
\noindent\begin{minipage}{0.5\textwidth}
  \flushleft
  {\scriptsize \textbf{LLVM}\\}
  {\scriptsize  \begin{tabular}{{l@{~~~~}l@{~~~~}r@{~~~~}r@{~~~~}c}}
\arrayrulecolor{darkgray}
\rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 &     75 &    0.92  &  0.01 & \quart{5}{6}{5}{49} \\
  1 &  75, weighting, Prune=25\% &    0.92  &  0.02 & \quart{0}{11}{5}{49} \\
  1 &  75, weighting &    0.93  &  0.02 & \quart{5}{11}{11}{49} \\
\hline  2 &     50 &    0.94  &  0.01 & \quart{16}{6}{16}{49} \\
  2 &  50, weighting, Prune=25\% &    0.95  &  0.0 & \quart{22}{0}{22}{49} \\
  2 &  50, weighting &    0.96  &  0.02 & \quart{22}{11}{27}{49} \\
\hline  3 &     25 &    0.98  &  0.01 & \quart{33}{5}{38}{49} \\
  3 &  25, weighting, Prune=25\% &    0.98  &  0.01 & \quart{33}{5}{38}{49} \\
  3 &  25, weighting &    0.98  &  0.0 & \quart{38}{0}{38}{49} \\
\hline  4 &  baseline &    1.0  &  0.0 & \quart{49}{0}{49}{49} \\
\hline \end{tabular}}
\end{minipage}


\noindent\begin{minipage}{0.5\textwidth}
  \flushleft
  {\scriptsize \textbf{SQL}\\}
  {\scriptsize  \begin{tabular}{{l@{~~~~}l@{~~~~}r@{~~~~}r@{~~~~}c}}
\arrayrulecolor{darkgray}
\rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 &      75 &    0.98  &  0.01 & \quart{0}{16}{16}{49} \\
  1 &  75, weighting, Prune=25\% &    0.98  &  0.0 & \quart{16}{0}{16}{49} \\
  1 &  75, weighting &    0.99  &  0.01 & \quart{16}{17}{33}{49} \\
  1 &  50, weighting, Prune=25\% &    0.99  &  0.0 & \quart{33}{0}{33}{49} \\
  1 &      50 &    0.99  &  0.0 & \quart{33}{0}{33}{49} \\
  1 &  50, weighting &    0.99  &  0.01 & \quart{33}{16}{33}{49} \\
  1 &  25, weighting, Prune=25\% &    1.0  &  0.0 & \quart{49}{0}{49}{49} \\
  1 &      25 &    1.0  &  0.0 & \quart{49}{0}{49}{49} \\
  1 &  25, weighting &    1.0  &  0.0 & \quart{49}{0}{49}{49} \\
  1 &  baseline &    1.0  &  0.0 & \quart{49}{0}{49}{49} \\
  \hline \end{tabular}}
\end{minipage}
\noindent\begin{minipage}{0.5\textwidth}
  \flushleft
  {\scriptsize \textbf{X264}\\}
  {\scriptsize  \begin{tabular}{{l@{~~~~}l@{~~~~}r@{~~~~}r@{~~~~}c}}
\arrayrulecolor{darkgray}
\rowcolor[gray]{.9} \textbf{Rank} & \textbf{Treatment} & \textbf{Median} & \textbf{IQR} & \\
  1 &  75, weighting, Prune=25\% &    0.73  &  0.05 & \quart{1}{9}{3}{49} \\
  1 &     75 &    0.74  &  0.05 & \quart{0}{8}{5}{49} \\
\hline  2 &  75, weighting &    0.81  &  0.04 & \quart{13}{7}{17}{49} \\
  2 &  50, weighting, Prune=25\% &    0.82  &  0.04 & \quart{17}{7}{18}{49} \\
  2 &     50 &    0.83  &  0.03 & \quart{17}{5}{20}{49} \\
\hline  3 &  50, weighting &    0.88  &  0.03 & \quart{25}{6}{29}{49} \\
\hline  4 &     25 &    0.9  &  0.03 & \quart{31}{5}{32}{49} \\
  4 &  25, weighting, Prune=25\% &    0.91  &  0.03 & \quart{32}{5}{34}{49} \\
\hline  5 &  25, weighting &    0.93  &  0.02 & \quart{36}{3}{37}{49} \\
\hline  6 &  baseline &    1.0  &  0.0 & \quart{49}{0}{49}{49} \\
\hline \end{tabular}}
\end{minipage}
\end{figure*}


% \section{Discussion}
\section{Threats to validity}

 

As with any empirical study, biases can affect the final results. Therefore, any
conclusions made from this work must be considered with the following issues in
mind.

{\em 1. Sampling bias} threatens any data mining experiment; i.e., what matters
there may not be true here. For example, the data sets used here comes from two sources
(Seigmund et al. and Jureczko et al.) and any biases in their selection procedures
threaten the validity of these results. 
That said,
the best we can do is define our methods and publicize our data and code so that other researchers can
try to repeat our results and, perhaps, point out a previously unknown bias
in our analysis. Hopefully, other researchers will emulate our methods in
order to repeat, refute, or improve our results. 

{\em 2. Learner bias: } For building the defect predictors in this study, we elected
to use  Random Forest  .
We chose Random Forest based on its reputation for having the better  performance of 
21 other learners for defect prediction~\cite{lessmann}.
Data mining is a
large and active field and any single study can only use a small
subset of the known classification algorithms.  

{\em 3. Evaluation bias:} The issue was discussed above in \tion{assess} where
we asserted that plans can be assessed via predictors-- with the all important
proviso that the planning should {\em not} be attempted if the predictors
are performing poorly.

%%\footnote{LL: It seems to me that $privacy$ in LACE approach could be compromised in the case of extremes if I had some knowledge of the parties. Consider a case where you know that a bunch of startups and Microsoft contributed to a private cache. Even with the random perturbation in MORPH, it will be REALLY obvious which defect data came from Windows vs. all of the others because Windows will have LOC orders of magnitude greater than anyone else, right?}
 
%XXXX everytime it says LACE, do you mean LACE2

%{\em 4. Comparison bias:}
%There are many privacy algorithms~\cite{Fung2010survey} and it would be difficult to 
%compare their performances (especially with various ways to measure privacy).
%In this work, we use LACE.

%{\em 5. Other Evaluation Bias:}
%The utility of  obfuscated data can be measured
%semantically (where the workload is unknown) or empirically (known workload
%e.g., classification or aggregate query answering). In this work we measure
%utility empirically for defect prediction only.


%\input{related}


\section{Conclusion}
% \section*{Acknowledgements}
 

\clearpage
\bibliography{how}{}
\bibliographystyle{IEEEtran}
\end{document}
